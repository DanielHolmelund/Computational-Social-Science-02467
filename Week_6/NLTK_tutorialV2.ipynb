{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9512e326",
   "metadata": {},
   "source": [
    "# Week 6 (Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b204a",
   "metadata": {},
   "source": [
    "## Exercise 6 & 30 from chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8612cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe561e09",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "* Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "a) [a-zA-Z]+\n",
    "<br>\n",
    "b) [A-Z][a-z]*\n",
    "<br>\n",
    "c) p[aeiou]{,2}t\n",
    "<br>\n",
    "d) \\d+(\\.\\d+)?\n",
    "<br>\n",
    "e) ([^aeiou][aeiou][^aeiou])*\n",
    "<br>\n",
    "f) \\w+|[^\\w\\s]+\n",
    "\n",
    "Test your answers using nltk.re_show()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5be52",
   "metadata": {},
   "source": [
    "A) The string that mathes this particular regex patterns are strings containing lower case and upper case letters with an arbitary length due to the plus operator which states one or more previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5669471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaAAA']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'aaaAAA'\n",
    "re.findall(r'[a-zA-Z]+', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1282fcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'A', 'A', 'Aaaaa', 'A']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'AAAAaaaaA'\n",
    "re.findall(r'[A-Z][a-z]*', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7c0e9",
   "metadata": {},
   "source": [
    "B) The above regex pattern mathes strings that has capital letters and then it also matches capital letters followed by a lower case letter, then the Kleene Closure operator ensures that we also get mathces where an uppercase letter is followed by multiple lower case letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c548f7e",
   "metadata": {},
   "source": [
    "word = 'Hej med dig'\n",
    "re.findall(r'[A-Z][a-z]*', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31fc66",
   "metadata": {},
   "source": [
    "Finds words that starts with an upper case or just upper case letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0abcd",
   "metadata": {},
   "source": [
    "C) Finds words that starts with a p and is then followed by two or less vowels from the set {aeiou} and ends with an t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95af61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'paat'\n",
    "re.findall(r'p[aeiou]{,2}t', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd45b0",
   "metadata": {},
   "source": [
    "D) It provides the fraction of real numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbfa3467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.001']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = '100.001'\n",
    "re.findall(r'\\d+(.\\d+)?', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b31a0a",
   "metadata": {},
   "source": [
    "E) The string that mathces this regex search is a consonant followed by a vowel and then a consonant, then the Kleene closure operator is used saying zero or more of previous, which in this case is the sequence stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d31c829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kak', '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'kak'\n",
    "re.findall(r'([^aeiou][aeiou][^aeiou])*', word ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e612f74",
   "metadata": {},
   "source": [
    "F) It finds all possible character strings or number strings, could be promising for tokenization purposes, given that it would seperate words and numbers into seperate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70b11a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'Jeg boede på lindegårdsvej nr 16' \n",
    "tokens = re.findall(r'\\w+|[^\\w\\s]+', word ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919146d",
   "metadata": {},
   "source": [
    "## Exercise 30\n",
    "◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a57c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', '...', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', '...', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "\n",
      "\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', '...', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', '...', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "... is no basis for a system of government.  Supreme executive power derives from\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print('\\n')\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fada5d",
   "metadata": {},
   "source": [
    "The most noticable difference is that the porter stemmer manage to change lying to lie instead of just removing the end. The lanchester stemmer also stems listen to list, which essentially changes the meaning of the word entirely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bbfb23",
   "metadata": {},
   "source": [
    "# Prelude to part 2 - Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99073004",
   "metadata": {},
   "source": [
    "Importing reddit submissions from wallstreetbets posted within January 1st and December 31st 2020 with content in English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7013cd4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wallstreet_subs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w8/wb7dt4yd6ys326nsmh5q64200000gn/T/ipykernel_29608/542984318.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wallstreet_subs.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[0;32m-> 1357\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wallstreet_subs.csv'"
     ]
    }
   ],
   "source": [
    "submissions = pd.read_csv(\"wallstreet_subs.csv\", sep=',')\n",
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a5c94",
   "metadata": {},
   "source": [
    "# Part 2 - Words that characterize stocks discussed on r/wallstreetbets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b9cc7",
   "metadata": {},
   "source": [
    "Exercise 2: Most discussed stocks in r/wallstreebets. GME is only one among many stocks people have discussed in r/wallstreetbets. In this exercise, we will find the most discussed stocks on wallstreetbets. Stocks are identified by their Ticker symbol. A Ticker symbol is nothing but a string consisting of letters and numbers, and is typically quite short. For example the Gamestop symbol is GME, Amazon is AMZN, Alphabet is GOOGL..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd55cc",
   "metadata": {},
   "source": [
    "1) Regex pattern to find words starting with a $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'I fucking love this $GME stuff'\n",
    "re.findall(r'\\$\\w+', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c71caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'I fucking love this $AMZN stuff'\n",
    "re.findall(r'\\$[a-zA-Z]+', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bffa8",
   "metadata": {},
   "source": [
    "2) Load the wallstreetbets submission dataset as a Pandas DataFrame and create a new column containing both the title and the textual content of each submission (as one long string). We refer to this as the text of the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions['text'] = submissions['title'] + ' ' + submissions['selftext']\n",
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81647728",
   "metadata": {},
   "source": [
    "3) For each submission, find all ticker symbols (those preceded by a dollar sign) contained in the text. Use the function re.findall, and the regular expression you created in point 1). Some tips for success:\n",
    "* Remove matches that are definetly not stock symbols\n",
    "* Convert all matches to uppercase\n",
    "* Remove the dollar sign at the beginning of the symbol (e.g. $gme → GME)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker(word: str)->list:\n",
    "    return re.findall(r'\\$[a-zA-Z]{1,5}', word)\n",
    "\n",
    "#Setting charachter length constraint due to the following: https://www.investopedia.com/terms/s/stocksymbol.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c789576",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions['ticker'] = submissions[\"text\"].map(get_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfa025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Initial number of submissions using ticker before filtering: {len(submissions[submissions.ticker.astype(bool)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(submissions))):\n",
    "    ticker_list = submissions.iloc[i, :]['ticker']\n",
    "    temp = []\n",
    "    for j in ticker_list:\n",
    "        temp.append(re.sub(r'\\$', '', j).upper())\n",
    "    submissions['ticker'][i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4753e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.iloc[100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to validate the approach by looking at the unique tickers\n",
    "(list(set([a for b in submissions.ticker.tolist() for a in b])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f971eca",
   "metadata": {},
   "source": [
    "By a quick inspection of the above results, it becomes evident that our current approach finds all words which start with an $, which also assemble common words. Examples which can be found in the above is words such as POPCORN, SUPERBOWL etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154211af",
   "metadata": {},
   "source": [
    "4) Create a list containing the top 15 Ticker Symbols by number of occurrences. GME should be among them. If it is not, check again your analysis and/or come talk to me. Google the top 15 symbols and find the corresponding company names. Are they known companies or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd22330",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist2 = nltk.FreqDist((list([a for b in submissions.ticker.tolist() for a in b])))\n",
    "fdist2.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist2.plot(15, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdef532",
   "metadata": {},
   "source": [
    "## Exercise 3: TF-IDF and the stocks discussed on r/wallstreetbets. \n",
    "The goal for this exercise is to find the words charachterizing each of the stocks discussed on r/wallstreetbets. We will focus on the top 15 stocks we idenfied in Exercise 2, and we will of course use TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2242488",
   "metadata": {},
   "source": [
    "First, check out the wikipedia page for TF-IDF. Explain in your own words the point of TF-IDF.\n",
    "* What does TF stand for?\n",
    "TF stands for term-frequency and essentially how frequent a token occurs in a document.\n",
    "<br>\n",
    "* What does IDF stand for?\n",
    "<br>\n",
    "IDF stands for inverse document frequence, which is the inverse of how often a token occurs in the corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da318b2f",
   "metadata": {},
   "source": [
    "Tokenize the text of each submission. Create a column tokens in your dataframe containing the tokens. Remember the bullets below for success.\n",
    "* If you dont' know what tokenization means, go back and read Chapter 3 again. The advice to go back and check Chapter 3 is valid for every cleaning step below.\n",
    "* Exclude punctuation.\n",
    "* Exclude URLs\n",
    "* Exclude stop words (if you don't know what stop words are, go back and read NLPP1e again).\n",
    "* Exclude numbers (since they're difficult to interpret in the word cloud).\n",
    "* Set everything to lower case.\n",
    "* Note that none of the above has to be perfect. And there's some room for improvisation. You can try using stemming. In my own first run the results didn't look so nice, because some submissions repeat certain words again and again and again, whereas other are very short. For that reason, I decided to use the unique set of words from each submission rather than each word in proportion to how it's actually used. Choices like that are up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm_pandas(tqdm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ebc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    #Removing urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    #Tokenize using nltk\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    #removing none letter characters and stop words\n",
    "    filtered_sentence = [w for w in word_tokens if w not in stop_words and w.isalpha()]\n",
    "    #Conduct stemming\n",
    "    processed_text = [porter.stem(t) for t in filtered_sentence]\n",
    "    return processed_text\n",
    "    \n",
    "submissions['tokens'] = submissions['text'].progress_apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552913f2",
   "metadata": {},
   "source": [
    "3) Find submissions discussing at least one of the top 15 stocks you identified above. To do so:\n",
    "* Create a function that finds the intersection between a list of tokens and your list of top 15 stocks. For example, your function applied to the tokens: \"[Here, TSLA, submission, GME]\" should return [\"TSLA\",\"GME\"]. (Optional: you can also try to included cases in which the list of tokens contains a company name among your top 15. For example the function applied to \"[Here, Gamestop, submission]\" could return ['GME'].)\n",
    "* Create a new column stock in your DataFrame, containing the output of your function applied to the text column. Values in this column should be lists.\n",
    "* Handle cases where one post discusses more than one stock by applying the function explode to the stock column. This will duplicate submissions associated to multiple stocks. After exploding, the values included in the stock column should be strings.\n",
    "* Handle cases where none of the selected stocks is discussed by replacing Nan values, for example with \"Other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb76ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top15_map = {\n",
    " 'tesla': 'TSLA',\n",
    " 'virgingalactic': 'SPCE',\n",
    " 'palantir': 'PLTR',\n",
    " 'microsoft': 'MSFT',\n",
    " 'tesla':'TSLA',\n",
    " 'amazon':'AMZN',\n",
    " 'zoom':'ZM',\n",
    " 'alibaba':'BABA',\n",
    " 'gamestop':'GME',\n",
    " 'disney':'DIS',\n",
    " 'boeing':'BA'   \n",
    "}\n",
    "\n",
    "fdist2 = nltk.FreqDist((list([a for b in submissions.ticker.tolist() for a in b])))\n",
    "top15 = [porter.stem(x[0]).lower() for x in fdist2.most_common(15)]\n",
    "print(top15)\n",
    "def inter_top_stock(tokens):\n",
    "    return list(set(tokens) & set(top15))\n",
    "\n",
    "submissions['stock'] = submissions['tokens'].apply(lambda x: inter_top_stock(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f8905",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions[submissions.stock.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526606b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploding stock column\n",
    "submissions = submissions.explode('stock')\n",
    "submissions['stock'] = submissions['stock'].fillna('other')\n",
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23857581",
   "metadata": {},
   "source": [
    "4) Now, we want to find out which words are important for each stock, so we're going to create several large documents, one for each stock. Each document includes all the tokens related to the same stock. We will also have a document including discussions that do not relate to the top 15 stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {}\n",
    "for i in top15:\n",
    "    temp = submissions[submissions['stock']==i]\n",
    "    corpora[i] = [item for sublist in temp['tokens'].tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135e325",
   "metadata": {},
   "source": [
    "5) Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within 5 stocks of your choice.\n",
    "\n",
    "   * Describe similarities and differences between the stocks.\n",
    "   * Why aren't the TFs not necessarily a good description of the stocks?\n",
    "   * Next, we calculate IDF for every word.\n",
    "   * What base logarithm did you use? Is that important?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We could use sklearn countvectorizer but lets try to use nltk\n",
    "#The 5 chosen are 'msft' 'gme' 'dis''aapl', 'amzn'\n",
    "stock5 = ['msft','gme','tsla','aapl','amzn']\n",
    "for i in stock5:\n",
    "    fdist = nltk.FreqDist(corpora[i])\n",
    "    print(f'The 5 most frequent words associated with {i.upper()} stock')\n",
    "    print(fdist.most_common(5))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36fe66",
   "metadata": {},
   "source": [
    "The similarities between common words among the five stocks is distictive for instance is the 'stock' among all of the most common words and the same goes for the stock's name itself. Furthermore, it also noticable that all mentions words which is related to earning money in some sense. This demonstrates one of the shortcomings of TF given that it only counts the number of occurrences, and we might have taken stop words into account by removing them, but at the current moment we have not accomodated for domain specific common words such as stock. This is where TF-IDF becomes relevant because the IDF exactly attempts to weight with the inverse of the document frequency meaning that less emphasis should be attributed to domain common words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41804d91",
   "metadata": {},
   "source": [
    "$$ IDF(t) = log(\\frac{Total\\ number\\ of\\ documents}{Number\\ of\\ documents\\ with\\ term\\ t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15992cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "temp = []\n",
    "for i in stock5:\n",
    "    temp.append(corpora[i])\n",
    "terms = set([item for sublist in temp for item in sublist])\n",
    "\n",
    "idf_dict = {}\n",
    "for term in tqdm(terms):\n",
    "    counter = 0\n",
    "    for i in stock5:\n",
    "        if term in set(corpora[i]):\n",
    "            counter += 1\n",
    "    idf_dict[term] = np.log((N)/(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb776914",
   "metadata": {},
   "source": [
    "Apparently then the log base is arbitrary given that it does not matter because of the following: https://stackoverflow.com/questions/56002611/when-to-use-which-base-of-log-for-tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae73a2",
   "metadata": {},
   "source": [
    "6) We're ready to calculate TF-IDF. Do that for the 5 stock of your choice.\n",
    "\n",
    "    * List the 10 top TF words for each stock.\n",
    "    * List the 10 top TF-IDF words for each stock.\n",
    "    * Are these 10 words more descriptive of the stock? If yes, what is it about IDF that makes the words more informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stock5:\n",
    "    print(f'###########\\n## {i} ##\\n##########')\n",
    "    fdist = nltk.FreqDist(corpora[i])\n",
    "    print(f'The 10 most frequent words associated with {i.upper()} stock:')\n",
    "    print(fdist.most_common(10))\n",
    "    fdist = dict(fdist)\n",
    "    print(f'The top 10 TF-idf words associated with {i.upper()} stock:')\n",
    "    tf_idf_dict = {}\n",
    "    for key in fdist:\n",
    "        tf_idf_dict[key] = idf_dict[key] * fdist[key]\n",
    "    print(sorted(tf_idf_dict.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813aa25",
   "metadata": {},
   "source": [
    "Exercise 4: The Wordcloud. It's time to visualize our results!\n",
    "\n",
    "* Install the WordCloud module.\n",
    "* Now, create word-cloud for each stock. Feel free to make it as fancy or non-fancy as you like.\n",
    "* Comment on the results. Are these words to be expected? Is there anything that is surprising?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2230cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using word frequencies\n",
    "for i in stock5:\n",
    "    wordcloud = WordCloud(background_color='white').generate_from_text(' '.join(corpora[i]))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using word tf-idf weighting\n",
    "for i in stock5:\n",
    "    fdist = dict(nltk.FreqDist(corpora[i]))\n",
    "    tf_idf_dict = {}\n",
    "    for key in fdist:\n",
    "        tf_idf_dict[key] = idf_dict[key] * fdist[key]\n",
    "    wordcloud = WordCloud(background_color='white').generate_from_frequencies(tf_idf_dict)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b866a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions[submissions['text'].str.lower().str.contains(\"powerfleet\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e565358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
